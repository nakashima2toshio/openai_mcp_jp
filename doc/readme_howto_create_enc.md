## MCP ã‚µãƒ¼ãƒãƒ¼ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã¨Streamlitã‚¢ãƒ—ãƒªæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰
#### ðŸ“‹ æ¦‚è¦
- ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€å„DBç³»MCPã‚µãƒ¼ãƒãƒ¼ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã—ã€Python Streamlitã‚¢ãƒ—ãƒªã§OpenAI APIã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚
### ðŸ› ï¸ äº‹å‰æº–å‚™
### 1. å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- bash# uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ï¼‰
```bash
pip install uv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
mkdir mcp-streamlit-demo
cd mcp-streamlit-demo

# Pythonç’°å¢ƒã‚’ä½œæˆ
uv init streamlit-mcp-app
cd streamlit-mcp-app
```

- å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
uv add streamlit openai python-dotenv pandas numpy requests redis psycopg2-binary elasticsearch qdrant-client pinecone-client
```
### 2. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
- bash# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
```bash
cat > .env << 'EOF'
# OpenAI API Key
OPENAI_API_KEY=your-openai-api-key-here

# Pinecone (å¿…è¦ã«å¿œã˜ã¦)
PINECONE_API_KEY=your-pinecone-api-key-here

# Redisè¨­å®š
REDIS_URL=redis://localhost:6379/0

# PostgreSQLè¨­å®š
PG_CONN_STR=postgresql://testuser:testpass@localhost:5432/testdb

# Elasticsearchè¨­å®š
ELASTIC_URL=http://localhost:9200

# Qdrantè¨­å®š
QDRANT_URL=http://localhost:6333
EOF
```

### ðŸš€ MCPã‚µãƒ¼ãƒãƒ¼ç¾¤ã®èµ·å‹•
#### 1. Docker Composeè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
```bash
# docker-compose.mcp-demo.yml
# version: '3.8'
#
services:
  # === ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç³» ===
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=testdb
      - POSTGRES_USER=testuser
      - POSTGRES_PASSWORD=testpass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-data/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U testuser -d testdb"]
      interval: 10s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # === MCP ã‚µãƒ¼ãƒãƒ¼ç¾¤ï¼ˆé–‹ç™ºä¸­ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰===
  # redis-mcp:
  #   build: https://github.com/redis/mcp-redis.git
  #   ports: ["8000:8000"]
  #   environment: [REDIS_URL=redis://redis:6379/0]
  #   depends_on:
  #     redis:
  #       condition: service_healthy

  # postgres-mcp:
  #   build: https://github.com/HenkDz/postgresql-mcp-server.git
  #   ports: ["8001:8000"]
  #   environment: [PG_CONN_STR=postgresql://testuser:testpass@postgres:5432/testdb]
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

volumes:
  redis_data:
  postgres_data:
  es_data:
  qdrant_data:

networks:
  default:
    name: mcp-demo-network
```

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•
- bash# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¾¤ã®ã¿èµ·å‹•ï¼ˆMCPã‚µãƒ¼ãƒãƒ¼ã¯å¾Œã§ï¼‰
```bash
docker-compose -f docker-compose.mcp-demo.yml up -d redis postgres elasticsearch qdrant
```

- èµ·å‹•ç¢ºèª
```bash
docker-compose -f docker-compose.mcp-demo.yml ps
```

### ðŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æŠ•å…¥
#### 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ
```bash
bashmkdir -p init-data scripts
```
#### 2. PostgreSQL ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿

```sql
-- init-data/postgres-init.sql
-- é¡§å®¢ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    age INTEGER,
    city VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ³¨æ–‡ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(id),
    product_name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) NOT NULL,
    quantity INTEGER NOT NULL,
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å•†å“ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    category VARCHAR(50),
    price DECIMAL(10,2) NOT NULL,
    stock_quantity INTEGER DEFAULT 0,
    description TEXT
);

-- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
INSERT INTO customers (name, email, age, city) VALUES
('ç”°ä¸­å¤ªéƒŽ', 'tanaka@example.com', 35, 'æ±äº¬'),
('ä½è—¤èŠ±å­', 'sato@example.com', 28, 'å¤§é˜ª'),
('éˆ´æœ¨ä¸€éƒŽ', 'suzuki@example.com', 42, 'åå¤å±‹'),
('é«˜æ©‹ç¾Žé¦™', 'takahashi@example.com', 31, 'ç¦å²¡'),
('æ¸¡è¾ºå¥ä¸€', 'watanabe@example.com', 29, 'æœ­å¹Œ');

INSERT INTO products (name, category, price, stock_quantity, description) VALUES
('ãƒŽãƒ¼ãƒˆPC', 'ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹', 89800.00, 15, 'é«˜æ€§èƒ½ãƒŽãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³'),
('ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³', 'ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹', 12800.00, 25, 'ãƒŽã‚¤ã‚ºã‚­ãƒ£ãƒ³ã‚»ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ã'),
('ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼', 'ã‚­ãƒƒãƒãƒ³å®¶é›»', 15600.00, 10, 'å…¨è‡ªå‹•ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼'),
('ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³', 8900.00, 20, 'ãƒ¬ã‚¶ãƒ¼è£½ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°'),
('ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³', 9800.00, 30, 'ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚·ãƒ¥ãƒ¼ã‚º');

INSERT INTO orders (customer_id, product_name, price, quantity) VALUES
(1, 'ãƒŽãƒ¼ãƒˆPC', 89800.00, 1),
(2, 'ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³', 12800.00, 2),
(3, 'ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼', 15600.00, 1),
(1, 'ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°', 8900.00, 1),
(4, 'ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼', 9800.00, 1),
(5, 'ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³', 12800.00, 1),
(2, 'ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼', 15600.00, 1);
```
3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
# scripts/setup_test_data.py
import redis
import json
import requests
from datetime import datetime
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import os
from dotenv import load_dotenv

load_dotenv()

def setup_redis_data():
    """Redisã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    print("ðŸ”´ Redis ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")

    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
    sessions = {
        'session:user1': {'user_id': 1, 'username': 'tanaka', 'login_time': '2024-01-15 10:30:00'},
        'session:user2': {'user_id': 2, 'username': 'sato', 'login_time': '2024-01-15 11:15:00'},
        'session:user3': {'user_id': 3, 'username': 'suzuki', 'login_time': '2024-01-15 09:45:00'}
    }

    for key, data in sessions.items():
        r.hset(key, mapping=data)

    # ã‚«ã‚¦ãƒ³ã‚¿æƒ…å ±
    counters = {
        'page_views': 1250,
        'user_registrations': 89,
        'sales_today': 15,
        'active_sessions': 3
    }

    for key, value in counters.items():
        r.set(f'counter:{key}', value)

    # å•†å“ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚»ãƒƒãƒˆï¼‰
    categories = ['ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹', 'ã‚­ãƒƒãƒãƒ³å®¶é›»', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³', 'ã‚¹ãƒãƒ¼ãƒ„', 'æœ¬ãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢']
    for category in categories:
        r.sadd('categories:all', category)

    # æœ€è¿‘ã®æ¤œç´¢å±¥æ­´ï¼ˆãƒªã‚¹ãƒˆï¼‰
    search_terms = ['ãƒŽãƒ¼ãƒˆPC', 'ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼', 'ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³', 'ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°', 'ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼']
    for term in search_terms:
        r.lpush('search:recent', term)

    # JSONå½¢å¼ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    user_profiles = {
        'profile:1': {
            'name': 'ç”°ä¸­å¤ªéƒŽ',
            'preferences': ['ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹', 'ã‚¬ã‚¸ã‚§ãƒƒãƒˆ'],
            'purchase_history': [{'product': 'ãƒŽãƒ¼ãƒˆPC', 'date': '2024-01-10'}]
        },
        'profile:2': {
            'name': 'ä½è—¤èŠ±å­',
            'preferences': ['ã‚­ãƒƒãƒãƒ³å®¶é›»', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³'],
            'purchase_history': [{'product': 'ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³', 'date': '2024-01-12'}]
        }
    }

    for key, profile in user_profiles.items():
        r.set(key, json.dumps(profile, ensure_ascii=False))

    print("âœ… Redis ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Œäº†!")
    return True

def setup_elasticsearch_data():
    """Elasticsearchã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    print("ðŸŸ¡ Elasticsearch ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")

    es_url = "http://localhost:9200"

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_mapping = {
        "mappings": {
            "properties": {
                "title": {"type": "text", "analyzer": "standard"},
                "content": {"type": "text", "analyzer": "standard"},
                "category": {"type": "keyword"},
                "author": {"type": "keyword"},
                "published_date": {"type": "date"},
                "tags": {"type": "keyword"},
                "view_count": {"type": "integer"}
            }
        }
    }

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    requests.put(f"{es_url}/blog_articles", json=index_mapping)

    # ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    articles = [
        {
            "title": "Pythonæ©Ÿæ¢°å­¦ç¿’å…¥é–€",
            "content": "Pythonã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤Žã«ã¤ã„ã¦èª¬æ˜Žã—ã¾ã™ã€‚scikit-learnã‚„pandasã‚’ä½¿ã£ã¦å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚’è¡Œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
            "category": "æŠ€è¡“",
            "author": "å±±ç”°å¤ªéƒŽ",
            "published_date": "2024-01-15",
            "tags": ["Python", "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹"],
            "view_count": 1250
        },
        {
            "title": "Dockerã‚³ãƒ³ãƒ†ãƒŠæ´»ç”¨è¡“",
            "content": "Dockerã‚’ä½¿ã£ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè·µçš„ãªæ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚Docker Composeã‚‚å«ã‚ã¦è§£èª¬ã€‚",
            "category": "æŠ€è¡“",
            "author": "éˆ´æœ¨èŠ±å­",
            "published_date": "2024-01-12",
            "tags": ["Docker", "DevOps", "ã‚³ãƒ³ãƒ†ãƒŠ"],
            "view_count": 980
        },
        {
            "title": "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®åŠ¹çŽ‡åŒ–",
            "content": "åœ¨å®…å‹¤å‹™ã§ç”Ÿç”£æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®å…·ä½“çš„ãªæ–¹æ³•ã¨ãƒ„ãƒ¼ãƒ«ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ”¹å–„ã®ã‚³ãƒ„ã‚‚ã€‚",
            "category": "ãƒ“ã‚¸ãƒã‚¹",
            "author": "ç”°ä¸­ä¸€éƒŽ",
            "published_date": "2024-01-10",
            "tags": ["ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯", "ç”Ÿç”£æ€§", "åƒãæ–¹æ”¹é©"],
            "view_count": 1580
        },
        {
            "title": "Streamlit Web ã‚¢ãƒ—ãƒªé–‹ç™º",
            "content": "Streamlitã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’å®Ÿä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚",
            "category": "æŠ€è¡“",
            "author": "ä½è—¤ç¾Žé¦™",
            "published_date": "2024-01-08",
            "tags": ["Streamlit", "Python", "Webã‚¢ãƒ—ãƒª"],
            "view_count": 750
        },
        {
            "title": "AIæ´»ç”¨ãƒ“ã‚¸ãƒã‚¹äº‹ä¾‹",
            "content": "ä¼æ¥­ã§ã®AIå°Žå…¥æˆåŠŸäº‹ä¾‹ã¨å¤±æ•—äº‹ä¾‹ã‚’åˆ†æžã—ã€åŠ¹æžœçš„ãªAIæ´»ç”¨ã®ãƒã‚¤ãƒ³ãƒˆã‚’è§£èª¬ã—ã¾ã™ã€‚",
            "category": "ãƒ“ã‚¸ãƒã‚¹",
            "author": "é«˜æ©‹å¥å¤ª",
            "published_date": "2024-01-05",
            "tags": ["AI", "DX", "ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥"],
            "view_count": 2100
        }
    ]

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæŠ•å…¥
    for i, article in enumerate(articles, 1):
        requests.post(f"{es_url}/blog_articles/_doc/{i}", json=article)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã‚’å¾…ã¤
    requests.post(f"{es_url}/blog_articles/_refresh")

    print("âœ… Elasticsearch ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Œäº†!")
    return True

def setup_qdrant_data():
    """Qdrantã«ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    print("ðŸŸ  Qdrant ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...")

    client = QdrantClient("localhost", port=6333)

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
    collection_name = "product_embeddings"

    try:
        client.delete_collection(collection_name=collection_name)
    except:
        pass

    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    )

    # å•†å“ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå®Ÿéš›ã¯äº‹å‰è¨ˆç®—æ¸ˆã¿ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ï¼‰
    # ã“ã“ã§ã¯ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨
    products = [
        {
            "id": 1,
            "name": "é«˜æ€§èƒ½ãƒŽãƒ¼ãƒˆPC",
            "category": "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹",
            "description": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ä½œæ¥­ã«æœ€é©ãªé«˜æ€§èƒ½ãƒŽãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³",
            "price": 89800,
            "vector": np.random.rand(384).tolist()
        },
        {
            "id": 2,
            "name": "ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³",
            "category": "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹",
            "description": "ãƒŽã‚¤ã‚ºã‚­ãƒ£ãƒ³ã‚»ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãã®é«˜éŸ³è³ªãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³",
            "price": 12800,
            "vector": np.random.rand(384).tolist()
        },
        {
            "id": 3,
            "name": "å…¨è‡ªå‹•ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼",
            "category": "ã‚­ãƒƒãƒãƒ³å®¶é›»",
            "description": "è±†ã‹ã‚‰æŒ½ã‘ã‚‹å…¨è‡ªå‹•ã‚¿ã‚¤ãƒ—ã®ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼",
            "price": 15600,
            "vector": np.random.rand(384).tolist()
        },
        {
            "id": 4,
            "name": "ãƒ¬ã‚¶ãƒ¼ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°",
            "category": "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",
            "description": "æœ¬é©è£½ã®é«˜ç´šãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°ã€ãƒŽãƒ¼ãƒˆPCã‚‚åŽç´å¯èƒ½",
            "price": 8900,
            "vector": np.random.rand(384).tolist()
        },
        {
            "id": 5,
            "name": "ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚·ãƒ¥ãƒ¼ã‚º",
            "category": "ã‚¹ãƒãƒ¼ãƒ„",
            "description": "è»½é‡ã§é€šæ°—æ€§ã®è‰¯ã„ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å°‚ç”¨ã‚·ãƒ¥ãƒ¼ã‚º",
            "price": 9800,
            "vector": np.random.rand(384).tolist()
        }
    ]

    # ãƒã‚¤ãƒ³ãƒˆæŠ•å…¥
    points = []
    for product in products:
        points.append(
            PointStruct(
                id=product["id"],
                vector=product["vector"],
                payload={
                    "name": product["name"],
                    "category": product["category"],
                    "description": product["description"],
                    "price": product["price"]
                }
            )
        )

    client.upsert(
        collection_name=collection_name,
        points=points
    )

    print("âœ… Qdrant ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Œäº†!")
    return True

def main():
    """å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    print("ðŸš€ MCP ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚’é–‹å§‹ã—ã¾ã™...\n")

    try:
        setup_redis_data()
        setup_elasticsearch_data()
        setup_qdrant_data()

        print("\nðŸŽ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ãŒå®Œäº†ã—ã¾ã—ãŸ!")
        print("\nðŸ“Š æŠ•å…¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿:")
        print("- Redis: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã€ã‚«ã‚¦ãƒ³ã‚¿ã€ã‚«ãƒ†ã‚´ãƒªã€æ¤œç´¢å±¥æ­´ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
        print("- PostgreSQL: é¡§å®¢ã€æ³¨æ–‡ã€å•†å“ãƒ‡ãƒ¼ã‚¿")
        print("- Elasticsearch: ãƒ–ãƒ­ã‚°è¨˜äº‹5ä»¶")
        print("- Qdrant: å•†å“ãƒ™ã‚¯ãƒˆãƒ«5ä»¶")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

    return True

if __name__ == "__main__":
    main()
```
4. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã®å®Ÿè¡Œ
```bash
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
uv run python scripts/setup_test_data.py

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šç¢ºèª
# PostgreSQL
docker-compose -f docker-compose.mcp-demo.yml exec postgres psql -U testuser -d testdb -c "SELECT * FROM customers;"

# Redis
docker-compose -f docker-compose.mcp-demo.yml exec redis redis-cli KEYS "*"

# Elasticsearch
curl "http://localhost:9200/blog_articles/_search?pretty"

# Qdrant
curl "http://localhost:6333/collections/product_embeddings/points?limit=5"
```

### ðŸ–¥ï¸ Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
1. ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
```python
# app.py
import streamlit as st
import openai
import os
import json
import pandas as pd
from dotenv import load_dotenv
import redis
import psycopg2
import requests
from datetime import datetime

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = os.getenv('OPENAI_API_KEY')
client = openai.OpenAI()

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="MCP ã‚µãƒ¼ãƒãƒ¼ ãƒ‡ãƒ¢",
    page_icon="ðŸ¤–",
    layout="wide"
)

st.title("ðŸ¤– MCP ã‚µãƒ¼ãƒãƒ¼ Ã— OpenAI API ãƒ‡ãƒ¢")
st.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§MCPã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ç¢ºèª
st.sidebar.header("ðŸ“Š MCP ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹")

def check_server_status():
    """å„ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    status = {}

    # Redis
    try:
        r = redis.Redis(host='localhost', port=6379, db=0)
        r.ping()
        status['Redis'] = "ðŸŸ¢ æŽ¥ç¶šOK"
    except:
        status['Redis'] = "ðŸ”´ æŽ¥ç¶šNG"

    # PostgreSQL
    try:
        conn = psycopg2.connect(os.getenv('PG_CONN_STR'))
        conn.close()
        status['PostgreSQL'] = "ðŸŸ¢ æŽ¥ç¶šOK"
    except:
        status['PostgreSQL'] = "ðŸ”´ æŽ¥ç¶šNG"

    # Elasticsearch
    try:
        response = requests.get('http://localhost:9200/_cluster/health', timeout=5)
        if response.status_code == 200:
            status['Elasticsearch'] = "ðŸŸ¢ æŽ¥ç¶šOK"
        else:
            status['Elasticsearch'] = "ðŸ”´ æŽ¥ç¶šNG"
    except:
        status['Elasticsearch'] = "ðŸ”´ æŽ¥ç¶šNG"

    # Qdrant
    try:
        response = requests.get('http://localhost:6333/health', timeout=5)
        if response.status_code == 200:
            status['Qdrant'] = "ðŸŸ¢ æŽ¥ç¶šOK"
        else:
            status['Qdrant'] = "ðŸ”´ æŽ¥ç¶šNG"
    except:
        status['Qdrant'] = "ðŸ”´ æŽ¥ç¶šNG"

    return status

# ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹è¡¨ç¤º
status = check_server_status()
for server, state in status.items():
    st.sidebar.text(f"{server}: {state}")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
tab1, tab2, tab3, tab4 = st.tabs(["ðŸ” ãƒ‡ãƒ¼ã‚¿ç¢ºèª", "ðŸ¤– AI ãƒãƒ£ãƒƒãƒˆ", "ðŸ“Š ç›´æŽ¥ã‚¯ã‚¨ãƒª", "âš™ï¸ è¨­å®š"])

with tab1:
    st.header("ðŸ“Š æŠ•å…¥ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ðŸ”´ Redis ãƒ‡ãƒ¼ã‚¿")
        if st.button("Redis ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
            try:
                r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
                st.write("**ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿:**")
                session_keys = r.keys('session:*')
                session_data = []
                for key in session_keys:
                    data = r.hgetall(key)
                    data['key'] = key
                    session_data.append(data)
                if session_data:
                    st.dataframe(pd.DataFrame(session_data))

                # ã‚«ã‚¦ãƒ³ã‚¿ãƒ‡ãƒ¼ã‚¿
                st.write("**ã‚«ã‚¦ãƒ³ã‚¿:**")
                counter_keys = r.keys('counter:*')
                counter_data = {}
                for key in counter_keys:
                    counter_data[key] = r.get(key)
                if counter_data:
                    st.json(counter_data)

            except Exception as e:
                st.error(f"RedisæŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

    with col2:
        st.subheader("ðŸŸ¦ PostgreSQL ãƒ‡ãƒ¼ã‚¿")
        if st.button("PostgreSQL ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
            try:
                conn = psycopg2.connect(os.getenv('PG_CONN_STR'))

                # é¡§å®¢ãƒ‡ãƒ¼ã‚¿
                st.write("**é¡§å®¢ãƒ‡ãƒ¼ã‚¿:**")
                df_customers = pd.read_sql("SELECT * FROM customers LIMIT 10", conn)
                st.dataframe(df_customers)

                # æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿
                st.write("**æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿:**")
                df_orders = pd.read_sql("SELECT * FROM orders LIMIT 10", conn)
                st.dataframe(df_orders)

                conn.close()
            except Exception as e:
                st.error(f"PostgreSQLæŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

    # Elasticsearch ã¨ Qdrant
    st.subheader("ðŸŸ¡ Elasticsearch ãƒ‡ãƒ¼ã‚¿")
    if st.button("Elasticsearch ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
        try:
            response = requests.get('http://localhost:9200/blog_articles/_search?size=10')
            if response.status_code == 200:
                data = response.json()
                articles = []
                for hit in data['hits']['hits']:
                    article = hit['_source']
                    article['_id'] = hit['_id']
                    articles.append(article)

                if articles:
                    df_articles = pd.DataFrame(articles)
                    st.dataframe(df_articles)
            else:
                st.error("Elasticsearch ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            st.error(f"ElasticsearchæŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

    st.subheader("ðŸŸ  Qdrant ãƒ‡ãƒ¼ã‚¿")
    if st.button("Qdrant ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
        try:
            response = requests.get('http://localhost:6333/collections/product_embeddings/points?limit=10')
            if response.status_code == 200:
                data = response.json()
                if 'result' in data and 'points' in data['result']:
                    products = []
                    for point in data['result']['points']:
                        product = point['payload']
                        product['id'] = point['id']
                        products.append(product)

                    if products:
                        df_products = pd.DataFrame(products)
                        st.dataframe(df_products)
        except Exception as e:
            st.error(f"QdrantæŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

with tab2:
    st.header("ðŸ¤– AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆMCPçµŒç”±ï¼‰")
    st.info("âš ï¸ æ³¨æ„: ã“ã®æ©Ÿèƒ½ã¯å®Ÿéš›ã®MCPã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ä½•ã‹è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šRedisã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’æ•™ãˆã¦ï¼‰"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # AIå¿œç­”ï¼ˆMCPã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒã—ã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        with st.chat_message("assistant"):
            response_placeholder = st.empty()

            try:
                # å®Ÿéš›ã®MCPå‘¼ã³å‡ºã—ï¼ˆã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹å ´åˆï¼‰
                # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
                response_text = f"""
ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨MCPã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒã—ã¦ã„ãªã„ãŸã‚ã€
ç›´æŽ¥çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ãŒã§ãã¾ã›ã‚“ã€‚

ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’ãŠä¼ãˆã—ã¾ã™ï¼š
- è³ªå•å†…å®¹: {prompt}
- åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿: Redisã€PostgreSQLã€Elasticsearchã€Qdrantã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥æ¸ˆã¿
- æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: MCPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„

MCPã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚³ãƒžãƒ³ãƒ‰:
```bash
# å€‹åˆ¥èµ·å‹•ã®ä¾‹
docker-compose -f docker-compose.mcp-demo.yml up -d redis-mcp postgres-mcp
            """

            response_placeholder.write(response_text)
            st.session_state.messages.append({"role": "assistant", "content": response_text})

        except Exception as e:
            error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
            response_placeholder.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})
with tab3:
st.header("ðŸ“Š ç›´æŽ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª")
query_type = st.selectbox("ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’é¸æŠž",
                         ["Redis", "PostgreSQL", "Elasticsearch", "Qdrant"])

if query_type == "Redis":
    st.subheader("ðŸ”´ Redis ã‚¯ã‚¨ãƒª")
    redis_command = st.text_input("Redisã‚³ãƒžãƒ³ãƒ‰", "KEYS *")

    if st.button("å®Ÿè¡Œ", key="redis_exec"):
        try:
            r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
            # ç°¡å˜ãªã‚³ãƒžãƒ³ãƒ‰ã®ã¿ã‚µãƒãƒ¼ãƒˆ
            if redis_command.startswith("KEYS"):
                result = r.keys(redis_command.split(" ", 1)[1] if " " in redis_command else "*")
            elif redis_command.startswith("GET"):
                key = redis_command.split(" ", 1)[1]
                result = r.get(key)
            else:
                result = "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚³ãƒžãƒ³ãƒ‰ã§ã™"

            st.code(str(result))
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

elif query_type == "PostgreSQL":
    st.subheader("ðŸŸ¦ PostgreSQL ã‚¯ã‚¨ãƒª")
    sql_query = st.text_area("SQLã‚¯ã‚¨ãƒª", "SELECT * FROM customers LIMIT 5;")

    if st.button("å®Ÿè¡Œ", key="pg_exec"):
        try:
            conn = psycopg2.connect(os.getenv('PG_CONN_STR'))
            df = pd.read_sql(sql_query, conn)
            st.dataframe(df)
            conn.close()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
with tab4:
st.header("âš™ï¸ è¨­å®š")
st.subheader("ðŸ”§ MCPã‚µãƒ¼ãƒãƒ¼èµ·å‹•")
st.code("""

#### MCPã‚µãƒ¼ãƒãƒ¼ç¾¤ã‚’èµ·å‹•
```bash
docker-compose -f docker-compose.mcp-demo.yml up -d redis-mcp postgres-mcp
å…¨ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
docker-compose -f docker-compose.mcp-demo.yml ps
ãƒ­ã‚°ç¢ºèª
docker-compose -f docker-compose.mcp-demo.yml logs -f redis-mcp
""")
st.subheader("ðŸŒ API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ")
st.write("MCPã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãŸã‚‰ä»¥ä¸‹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½:")
st.json({
    "Redis MCP": "http://localhost:8000/mcp",
    "PostgreSQL MCP": "http://localhost:8001/mcp",
    "Elasticsearch MCP": "http://localhost:8002/mcp",
    "Qdrant MCP": "http://localhost:8003/mcp"
})

st.subheader("ðŸ”‘ ç’°å¢ƒå¤‰æ•°")
env_vars = {
    "OPENAI_API_KEY": "è¨­å®šæ¸ˆã¿" if os.getenv('OPENAI_API_KEY') else "æœªè¨­å®š",
    "REDIS_URL": os.getenv('REDIS_URL', 'æœªè¨­å®š'),
    "PG_CONN_STR": "è¨­å®šæ¸ˆã¿" if os.getenv('PG_CONN_STR') else "æœªè¨­å®š"
}
st.json(env_vars)
ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("ðŸš€ MCP Demo App - OpenAI API Ã— MCP ã‚µãƒ¼ãƒãƒ¼é€£æºã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
```

### 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•

```bash
# Streamlitã‚¢ãƒ—ãƒªã‚’èµ·å‹•
uv run streamlit run app.py

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:8501 ã«ã‚¢ã‚¯ã‚»ã‚¹
ðŸŽ¯ å®Ÿè¡Œæ‰‹é †ã¾ã¨ã‚
1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
bash# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
mkdir mcp-streamlit-demo && cd mcp-streamlit-demo
uv init streamlit-mcp-app && cd streamlit-mcp-app

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv add streamlit openai python-dotenv pandas numpy requests redis psycopg2-binary elasticsearch qdrant-client

# ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼‰
# OPENAI_API_KEY ã‚’å®Ÿéš›ã®ã‚­ãƒ¼ã«è¨­å®š
2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•
bash# Docker Composeè¨­å®šã‚’ã‚³ãƒ”ãƒ¼ã—ã¦é…ç½®
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¾¤ã‚’èµ·å‹•
docker-compose -f docker-compose.mcp-demo.yml up -d redis postgres elasticsearch qdrant

# èµ·å‹•ç¢ºèª
docker-compose -f docker-compose.mcp-demo.yml ps
3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
bash# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆãƒ»é…ç½®
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Ÿè¡Œ
uv run python scripts/setup_test_data.py
4. Streamlitã‚¢ãƒ—ãƒªèµ·å‹•
bash# ã‚¢ãƒ—ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆãƒ»é…ç½®
# ã‚¢ãƒ—ãƒªèµ·å‹•
uv run streamlit run app.py
5. MCPã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
bash# å®Ÿéš›ã®MCPæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆ
docker-compose -f docker-compose.mcp-demo.yml up -d redis-mcp postgres-mcp es-mcp qdrant-mcp
ðŸŽ‰ å®Œæˆï¼
ã“ã‚Œã§ã€å„ç¨®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒæŠ•å…¥ã•ã‚Œã€Streamlitã‚¢ãƒ—ãƒªã§ç¢ºèªãƒ»æ“ä½œã§ãã‚‹ç’°å¢ƒãŒæ•´ã„ã¾ã—ãŸã€‚
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:

MCPã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿéš›ã«èµ·å‹•ã—ã¦OpenAI APIã¨ã®é€£æºã‚’ãƒ†ã‚¹ãƒˆ
ã‚ˆã‚Šè¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚„åˆ†æžæ©Ÿèƒ½ã‚’è¿½åŠ 
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ©Ÿèƒ½ã®å®Ÿè£…
ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼æ©Ÿèƒ½ã®è¿½åŠ 