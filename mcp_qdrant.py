# mcp_qdrant.py - MCPçµŒç”±ã§è‡ªç„¶è¨€èªã§Qdrantã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹Streamlitã‚¢ãƒ—ãƒª
# streamlit run mcp_qdrant.py --server.port=8505
# å‰æ: Qdrant MCPã‚µãƒ¼ãƒãƒ¼ãŒãƒãƒ¼ãƒˆ8003ã§èµ·å‹•ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

import streamlit as st
import os
import pandas as pd
import json
import time
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv
import plotly.express as px
import plotly.graph_objects as go

# Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨Embeddingé–¢é€£
try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False
    st.error("Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå¿…è¦ã§ã™: pip install qdrant-client")

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_api import OpenAIClient, MessageManager, config, logger
from helper_mcp import MCPSessionManager
from helper_st import UIHelper


class MCPQdrantManager:
    """MCPå¯¾å¿œQdrantãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œç®¡ç† (ãƒ‡ãƒ¢ç”¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰)"""
    
    def __init__(self, mcp_server_url: str = "http://localhost:8003/mcp", qdrant_url: str = None):
        self.mcp_server_url = mcp_server_url
        self.qdrant_url = qdrant_url or os.getenv('QDRANT_URL', 'http://localhost:6333')
        self.collections_info = None
        self._cached_collections_info = None
        self._qdrant_client = None
        
        if QDRANT_AVAILABLE:
            try:
                self._qdrant_client = QdrantClient(url=self.qdrant_url)
            except Exception as e:
                logger.error(f"Qdrant client initialization error: {e}")
    
    def get_collections_info(self) -> Dict[str, Any]:
        """MCPã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—"""
        if self._cached_collections_info is not None:
            return self._cached_collections_info
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆQdrantåˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        default_collections = {
            "documents": {
                "description": "æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
                "vector_size": 384,
                "distance": "cosine",
                "fields": ["title", "content", "category", "timestamp", "author"],
                "sample_count": 100
            },
            "products": {
                "description": "å•†å“ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ (product_embeddings)",
                "vector_size": 384,
                "distance": "cosine", 
                "fields": ["ID", "name", "category", "description", "price"],
                "sample_count": 5
            },
            "product_embeddings": {
                "description": "å•†å“ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
                "vector_size": 384,
                "distance": "cosine", 
                "fields": ["ID", "name", "category", "description", "price"],
                "sample_count": 5
            },
            "knowledge_base": {
                "description": "çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
                "vector_size": 384,
                "distance": "cosine",
                "fields": ["topic", "content", "tags", "difficulty_level", "source"],
                "sample_count": 200
            }
        }
        
        # å®Ÿéš›ã®Qdrantã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã—ã¦ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        if self._qdrant_client:
            try:
                actual_collections = {}
                collections_response = self._qdrant_client.get_collections()
                
                for collection in collections_response.collections:
                    collection_name = collection.name
                    collection_info = self._qdrant_client.get_collection(collection_name)
                    
                    actual_collections[collection_name] = {
                        "description": f"{collection_name}ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
                        "vector_size": collection_info.config.params.vectors.size,
                        "distance": collection_info.config.params.vectors.distance.name.lower(),
                        "fields": ["id", "payload", "vector"],  # Qdrantã®åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                        "sample_count": collection_info.points_count or 0
                    }
                
                if actual_collections:
                    self._cached_collections_info = actual_collections
                    return actual_collections
                    
            except Exception as e:
                logger.warning(f"Could not fetch actual Qdrant collections: {e}")
        
        self._cached_collections_info = default_collections
        return default_collections


class MCPVectorQueryProcessor:
    """MCPçµŒç”±ã§è‡ªç„¶è¨€èªãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã™ã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µ"""
    
    def __init__(self, openai_client: OpenAIClient, qdrant_manager: MCPQdrantManager):
        self.openai_client = openai_client
        self.qdrant_manager = qdrant_manager
        self.collections_info = qdrant_manager.get_collections_info()
    
    def build_mcp_prompt(self, user_query: str) -> List[Dict[str, str]]:
        """MCPçµŒç”±ã§ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        collections_text = self._format_collections_info()
        
        system_prompt = f"""ã‚ãªãŸã¯Qdrant MCPã‚µãƒ¼ãƒãƒ¼ã¨é€£æºã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªã«ã‚ˆã‚‹è³ªå•ã‚’ç†è§£ã—ã€é©åˆ‡ãªãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ã€Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã€‘
{collections_text}

ã€MCPæ“ä½œã«ã¤ã„ã¦ã€‘
- Qdrant MCPã‚µãƒ¼ãƒãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã§ã™
- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€é¡ä¼¼åº¦æ¤œç´¢ãŒå¯èƒ½ã§ã™
- çµæœã¯JSONå½¢å¼ã§è¿”ã•ã‚Œã¾ã™
- æ—¥æœ¬èªã§ã®è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„

ã€å¿œç­”å½¢å¼ã€‘
MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ã‚¨ãƒªã—ã€çµæœã‚’æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
    
    def _format_collections_info(self) -> str:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        collections_text = ""
        for collection_name, info in self.collections_info.items():
            collections_text += f"\nã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}ã€‘\n"
            collections_text += f"  - èª¬æ˜: {info['description']}\n"
            collections_text += f"  - ãƒ™ã‚¯ãƒˆãƒ«ã‚µã‚¤ã‚º: {info['vector_size']}\n"
            collections_text += f"  - è·é›¢é–¢æ•°: {info['distance']}\n"
            collections_text += f"  - ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {', '.join(info['fields'])}\n"
            collections_text += f"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {info['sample_count']}\n"
        
        return collections_text
    
    def execute_mcp_query(self, user_query: str, model: str = "gpt-5-mini") -> Tuple[bool, List[Dict], str]:
        """MCPå¯¾å¿œãƒ‡ãƒ¢: AIç”Ÿæˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸQdrantã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        try:
            # Step 1: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã‹ã‚‰æ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆ (MCPæ¦‚å¿µã®ãƒ‡ãƒ¢)
            search_strategy, explanation = self._generate_search_strategy_via_ai(user_query, model)
            
            if not search_strategy:
                return False, [], "æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
            
            # Step 2: Qdrantã§æ¤œç´¢å®Ÿè¡Œ (MCPã‚µãƒ¼ãƒãƒ¼ä»£æ›¿)
            results = self._execute_vector_search_directly(search_strategy, user_query)
            
            # Step 3: çµæœã®èª¬æ˜ç”Ÿæˆ
            if results:
                response_text = f"**æ¤œç´¢æˆ¦ç•¥**: {search_strategy['description']}\n\n**å®Ÿè¡Œçµæœ**: {len(results)}ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚\n\n{explanation}"
            else:
                response_text = f"**æ¤œç´¢æˆ¦ç•¥**: {search_strategy['description']}\n\n**å®Ÿè¡Œçµæœ**: ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n{explanation}"
            
            return True, results, response_text
            
        except Exception as e:
            logger.error(f"MCP vector query error: {e}")
            return False, [], f"MCPãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
    
    def _generate_search_strategy_via_ai(self, user_query: str, model: str) -> Tuple[Dict, str]:
        """AI ã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ (MCPæ¦‚å¿µã®ãƒ‡ãƒ¢)"""
        try:
            collections_text = self._format_collections_info()
            
            strategy_prompt = f"""ä»¥ä¸‹ã®Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã€‘
{collections_text}

ã€åˆ¶ç´„ã€‘
- é©åˆ‡ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„
- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šã—ã¦ãã ã•ã„
- å®‰å…¨ãªæ¤œç´¢æˆ¦ç•¥ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- JSONå½¢å¼ã§æˆ¦ç•¥ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€è³ªå•ã€‘: {user_query}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "collection": "é©åˆ‡ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å",
    "query_type": "semantic_search|filter_search|hybrid_search",
    "search_text": "æ¤œç´¢ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ",
    "filters": {{}},
    "limit": 10,
    "description": "æ¤œç´¢æˆ¦ç•¥ã®èª¬æ˜"
}}"""
            
            response = self.openai_client.create_response(
                input=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å°‚é–€å®¶ã§ã™ã€‚åŠ¹ç‡çš„ã§å®‰å…¨ãªQdrantæ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": strategy_prompt}
                ],
                model=model
            )
            
            from helper_api import ResponseProcessor
            texts = ResponseProcessor.extract_text(response)
            
            if texts:
                strategy_text = self._clean_json_response(texts[0])
                try:
                    strategy = json.loads(strategy_text)
                    explanation = f"è³ªå•ã€{user_query}ã€ã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
                    return strategy, explanation
                except json.JSONDecodeError:
                    # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    fallback_strategy = {
                        "collection": "documents",
                        "query_type": "semantic_search",
                        "search_text": user_query,
                        "filters": {},
                        "limit": 10,
                        "description": "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¤œç´¢æˆ¦ç•¥"
                    }
                    return fallback_strategy, "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
            
            return {}, "æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
            
        except Exception as e:
            logger.error(f"Search strategy generation error: {e}")
            return {}, f"æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def _clean_json_response(self, text: str) -> str:
        """AIå¿œç­”ã‹ã‚‰JSONã‚’æŠ½å‡ºãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        import re
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
        text = re.sub(r'```json\n?', '', text)
        text = re.sub(r'```\n?', '', text)
        text = text.strip()
        
        # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            return text[start_idx:end_idx+1]
        
        return text
    
    def _execute_vector_search_directly(self, strategy: Dict, user_query: str) -> List[Dict]:
        """Qdrantã§ç›´æ¥ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ (MCPã‚µãƒ¼ãƒãƒ¼ä»£æ›¿)"""
        try:
            if not self.qdrant_manager._qdrant_client:
                # Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                return self._generate_mock_search_results(strategy, user_query)
            
            collection_name = strategy.get('collection', 'documents')
            search_text = strategy.get('search_text', user_query)
            limit = strategy.get('limit', 10)
            
            # æ¤œç´¢ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆOpenAI embeddingsä½¿ç”¨ï¼‰
            search_vector = self._get_embedding(search_text)
            
            if not search_vector:
                return self._generate_mock_search_results(strategy, user_query)
            
            # Qdrantã§æ¤œç´¢å®Ÿè¡Œï¼ˆæ–°ã—ã„APIã‚’ä½¿ç”¨ï¼‰
            search_result = self.qdrant_manager._qdrant_client.query_points(
                collection_name=collection_name,
                query=search_vector,
                limit=limit,
                with_payload=True
            )
            
            # çµæœã‚’Dictå½¢å¼ã«å¤‰æ›
            results = []
            for point in search_result:
                result_dict = {
                    'id': point.id,
                    'score': float(point.score),
                    'payload': point.payload or {}
                }
                results.append(result_dict)
            
            return results
                    
        except Exception as e:
            logger.error(f"Direct vector search execution error: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return self._generate_mock_search_results(strategy, user_query)
    
    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
        try:
            response = self.openai_client.client.embeddings.create(
                input=text,
                model="text-embedding-3-small"
            )
            embedding = response.data[0].embedding
            
            # Qdrantã®ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã«åˆã‚ã›ã¦èª¿æ•´ï¼ˆ384æ¬¡å…ƒã«åˆ‡ã‚Šè©°ã‚ï¼‰
            if len(embedding) > 384:
                embedding = embedding[:384]
            elif len(embedding) < 384:
                # å¿…è¦ã«å¿œã˜ã¦ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                embedding.extend([0.0] * (384 - len(embedding)))
            
            return embedding
        except Exception as e:
            logger.error(f"Embedding generation error: {e}")
            return None
    
    def _generate_mock_search_results(self, strategy: Dict, user_query: str) -> List[Dict]:
        """ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœã‚’ç”Ÿæˆï¼ˆQdrantæ¥ç¶šã§ããªã„å ´åˆï¼‰"""
        collection_name = strategy.get('collection', 'documents')
        mock_results = []
        
        if collection_name == 'documents':
            mock_results = [
                {
                    'id': 'doc_001',
                    'score': 0.95,
                    'payload': {
                        'title': 'AIæŠ€è¡“æ¦‚è¦',
                        'content': 'AIæŠ€è¡“ã«é–¢ã™ã‚‹åŸºæœ¬çš„ãªèª¬æ˜æ–‡æ›¸',
                        'category': 'technology',
                        'timestamp': '2024-01-15T10:00:00Z',
                        'author': 'AIç ”ç©¶ãƒãƒ¼ãƒ '
                    }
                },
                {
                    'id': 'doc_002', 
                    'score': 0.88,
                    'payload': {
                        'title': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ',
                        'content': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®åŸºæœ¬åŸå‰‡ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹',
                        'category': 'database',
                        'timestamp': '2024-01-20T14:30:00Z',
                        'author': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å°‚é–€å®¶'
                    }
                }
            ]
        elif collection_name in ['products', 'product_embeddings']:
            mock_results = [
                {
                    'id': 1,
                    'score': 0.92,
                    'payload': {
                        'ID': 1,
                        'name': 'é«˜æ€§èƒ½ãƒãƒ¼ãƒˆPC',
                        'description': 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ä½œæ¥­ã«æœ€é©ãªé«˜æ€§èƒ½ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³',
                        'category': 'ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹',
                        'price': 89800
                    }
                },
                {
                    'id': 4,
                    'score': 0.75,
                    'payload': {
                        'ID': 4,
                        'name': 'ãƒ¬ã‚¶ãƒ¼ãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°',
                        'description': 'æœ¬é©è£½ã®é«˜ç´šãƒ“ã‚¸ãƒã‚¹ãƒãƒƒã‚°ã€ãƒãƒ¼ãƒˆPCã‚‚åç´å¯èƒ½',
                        'category': 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³',
                        'price': 8900
                    }
                },
                {
                    'id': 2,
                    'score': 0.65,
                    'payload': {
                        'ID': 2,
                        'name': 'ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³',
                        'description': 'ãƒã‚¤ã‚ºã‚­ãƒ£ãƒ³ã‚»ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãã®é«˜éŸ³è³ªãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³',
                        'category': 'ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹',
                        'price': 12800
                    }
                }
            ]
        elif collection_name == 'knowledge_base':
            mock_results = [
                {
                    'id': 'kb_001',
                    'score': 0.90,
                    'payload': {
                        'topic': 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åŸºç¤',
                        'content': 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µã¨å®Ÿè·µ',
                        'tags': ['programming', 'basics', 'education'],
                        'difficulty_level': 'beginner',
                        'source': 'æ•™è‚²ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ '
                    }
                }
            ]
        
        return mock_results[:strategy.get('limit', 5)]
    
    def explain_results(self, query: str, results: List[Dict], model: str = "gpt-4o-mini") -> str:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’è‡ªç„¶è¨€èªã§èª¬æ˜"""
        if not results:
            return "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        try:
            # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
            result_summary = f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: {len(results)}ä»¶\n"
            result_summary += "\næ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿:\n"
            
            for i, result in enumerate(results[:3], 1):
                score = result.get('score', 0)
                payload = result.get('payload', {})
                result_summary += f"{i}. [é¡ä¼¼åº¦: {score:.2f}] {payload}\n"
            
            if len(results) > 3:
                result_summary += f"... (ä»–{len(results)-3}ä»¶)"
            
            messages = [
                {
                    "role": "system", 
                    "content": "ã‚ãªãŸã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚æ¤œç´¢çµæœã®å†…å®¹ã¨é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user", 
                    "content": f"ä»¥ä¸‹ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã«ã¤ã„ã¦ã€ã‚ã‹ã‚Šã‚„ã™ãæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„:\n\nè³ªå•: {query}\n\n{result_summary}"
                }
            ]
            
            response = self.openai_client.create_response(
                input=messages,
                model=model
            )
            
            from helper_api import ResponseProcessor
            texts = ResponseProcessor.extract_text(response)
            
            if texts:
                return texts[0].strip()
            else:
                return f"{len(results)}ä»¶ã®é¡ä¼¼çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚"
                
        except Exception as e:
            logger.error(f"Result explanation error: {e}")
            return f"{len(results)}ä»¶ã®é¡ä¼¼çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚"


class NaturalLanguageVectorInterface:
    """è‡ªç„¶è¨€èªãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
        load_dotenv()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
        MCPSessionManager.init_session()
        self._init_session_state()
        
        # MCP Qdrantæ¥ç¶š
        mcp_server_url = os.getenv('QDRANT_MCP_URL', 'http://localhost:8003/mcp')
        self.qdrant_manager = MCPQdrantManager(mcp_server_url)
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        try:
            self.openai_client = OpenAIClient()
            self.query_processor = MCPVectorQueryProcessor(self.openai_client, self.qdrant_manager)
        except Exception as e:
            st.error(f"OpenAI APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()
    
    def _init_session_state(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
        defaults = {
            'selected_model': 'gpt-5-mini',
            'query_history': [],
            'current_results': None,
            'current_explanation': "",
            'collections_loaded': False
        }
        
        for key, value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = value
    
    def get_available_models(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        return config.get("models.available", [
            "gpt-5", "gpt-5-mini", "gpt-5-nano",
            "gpt-4.1", "gpt-4.1-mini", 
            "gpt-4o", "gpt-4o-mini",
            "o3", "o4-mini"
        ])
    
    def get_query_suggestions(self) -> List[str]:
        """è‡ªç„¶è¨€èªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¯ã‚¨ãƒªã®å€™è£œã‚’è¿”ã™"""
        return [
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ä½œæ¥­ã«é©ã—ãŸãƒãƒ¼ãƒˆPCã‚’æ¢ã—ã¦",
            "é«˜éŸ³è³ªãªãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³ã‚’æ¤œç´¢ã—ã¦",
            "å…¨è‡ªå‹•ã®ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’è¦‹ã¤ã‘ã¦",
            "ãƒ“ã‚¸ãƒã‚¹ç”¨ã®æœ¬é©ãƒãƒƒã‚°ã‚’æ¢ã—ã¦",
            "ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ç”¨ã®è»½é‡ã‚·ãƒ¥ãƒ¼ã‚ºã‚’æ¤œç´¢ã—ã¦",
            "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹è£½å“ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¦",
            "ã‚­ãƒƒãƒãƒ³å®¶é›»ã®å•†å“ã‚’æ¢ã—ã¦",
            "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³é–¢é€£ã®å•†å“ã‚’æ¤œç´¢ã—ã¦",
            "ã‚¹ãƒãƒ¼ãƒ„ç”¨å“ã‚’è¦‹ã¤ã‘ã¦",
            "10000å††ä»¥ä¸‹ã®å•†å“ã‚’æ¢ã—ã¦",
            "é«˜ä¾¡æ ¼å¸¯ã®å•†å“ã‚’æ¤œç´¢ã—ã¦",
            "é¡ä¼¼ã—ãŸå•†å“ã‚’æ¢ã—ã¦"
        ]
    
    def create_sidebar(self):
        """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½œæˆ"""
        st.sidebar.title("ğŸ” è¨­å®š")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        available_models = self.get_available_models()
        selected_model = st.sidebar.selectbox(
            "ğŸ¯ OpenAI ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            options=available_models,
            index=available_models.index("gpt-5-mini") if "gpt-5-mini" in available_models else 0,
            help="è‡ªç„¶è¨€èªã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¸ã®å¤‰æ›ç²¾åº¦ã«å½±éŸ¿ã—ã¾ã™"
        )
        st.session_state.selected_model = selected_model
        
        st.sidebar.markdown("---")
        
        # Qdrantæƒ…å ±
        st.sidebar.subheader("ğŸ—„ï¸ Qdrantæƒ…å ±")
        if st.sidebar.button("ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ›´æ–°"):
            self.query_processor.collections_info = self.qdrant_manager.get_collections_info()
            st.session_state.collections_loaded = True
            st.sidebar.success("ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º
        with st.sidebar.expander("ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã‚’è¡¨ç¤º"):
            collections_info = self.query_processor.collections_info
            for collection_name, info in collections_info.items():
                st.write("---")
                st.write(f"**{collection_name}**")
                st.write(f"ğŸ“ {info['description']}")
                st.write(f"ğŸ“ ãƒ™ã‚¯ãƒˆãƒ«ã‚µã‚¤ã‚º: {info['vector_size']}")
                st.write(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {info['sample_count']}")
        
        st.sidebar.markdown("---")
        
        # ã‚¯ã‚¨ãƒªå±¥æ­´
        if st.session_state.query_history:
            st.sidebar.subheader("ğŸ“ ã‚¯ã‚¨ãƒªå±¥æ­´")
            for i, (query, _) in enumerate(reversed(st.session_state.query_history[-5:])):
                if st.sidebar.button(f"{query[:30]}...", key=f"history_{i}"):
                    st.session_state.current_query = query
    
    def create_main_interface(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ"""
        st.title("ğŸ” MCPçµŒç”±ã§Qdrantã‚¢ã‚¯ã‚»ã‚¹")
        
        with st.expander("ğŸ”— OpenAI API éƒ¨åˆ†"):
            st.code("""
            
  Input (å…¥åŠ›)

  # Line: OpenAI API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ (æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ)
  response = self.openai_client.create_response(
      input=[
          {"role": "system", "content": 
  "ã‚ãªãŸã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å°‚é–€å®¶ã§ã™ã€‚åŠ¹ç‡çš„ã§å®‰å…¨ãªQdrantæ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"},
          {"role": "user", "content": strategy_prompt}
      ],
      model=model
  )

  # Line: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
  response = self.openai_client.client.embeddings.create(
      input=text,
      model="text-embedding-3-small"
  )

  Process (å‡¦ç†)

  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ã®ç”Ÿæˆ
  def _generate_search_strategy_via_ai(self, user_query: str, model: str):
      # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
      strategy_prompt = f"ä»¥ä¸‹ã®Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
  ã€Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã€‘{collections_text}
  ã€åˆ¶ç´„ã€‘- é©åˆ‡ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„
  ã€è³ªå•ã€‘: {user_query}"

      # OpenAI API ã§æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ
      response = self.openai_client.create_response(...)

      # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
      texts = ResponseProcessor.extract_text(response)
      strategy = json.loads(strategy_text)

  Output (å‡ºåŠ›)
    
      # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®å‡¦ç†
      search_result = qdrant_client.search(
          collection_name=collection_name,
          query_vector=search_vector,
          limit=limit,
          with_payload=True
      )
      
      results = []
      for point in search_result:
          result_dict = {
              'id': point.id,
              'score': float(point.score),
              'payload': point.payload or {}
          }
          results.append(result_dict)
            """)
            
        with st.expander("ğŸ”— MCP (Model Context Protocol) éƒ¨åˆ†"):
            st.code("""
            
  Input (å…¥åŠ›)

  # MCPãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨)
  def build_mcp_prompt(self, user_query: str) -> List[Dict[str, str]]:
      system_prompt = f"ã‚ãªãŸã¯Qdrant MCPã‚µãƒ¼ãƒãƒ¼ã¨é€£æºã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
  ã€Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã€‘{collections_text}
  ã€MCPæ“ä½œã«ã¤ã„ã¦ã€‘
  - Qdrant MCPã‚µãƒ¼ãƒãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã§ã™
  - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€é¡ä¼¼åº¦æ¤œç´¢ãŒå¯èƒ½ã§ã™
  - çµæœã¯JSONå½¢å¼ã§è¿”ã•ã‚Œã¾ã™"

      return [
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": user_query}
      ]

  Process (å‡¦ç†)

  # MCPãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œå‡¦ç†ï¼ˆãƒ‡ãƒ¢ç‰ˆï¼‰
  def execute_mcp_query(self, user_query: str, model: str = "gpt-5-mini"):
      # Step 1: AI ã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ (MCPæ¦‚å¿µã®ãƒ‡ãƒ¢)
      search_strategy, explanation = self._generate_search_strategy_via_ai(user_query, model)

      # Step 2: Qdrantã§ç›´æ¥å®Ÿè¡Œ (MCPã‚µãƒ¼ãƒãƒ¼ä»£æ›¿)
      results = self._execute_vector_search_directly(search_strategy, user_query)

      # Step 3: çµæœã®èª¬æ˜ç”Ÿæˆ
      response_text = f"**æ¤œç´¢æˆ¦ç•¥**: {search_strategy['description']}"

  Output (å‡ºåŠ›)

  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹
  def _execute_vector_search_directly(self, strategy: Dict, user_query: str):
      search_result = self.qdrant_manager._qdrant_client.search(
          collection_name=collection_name,
          query_vector=search_vector,
          limit=limit,
          with_payload=True
      )
      
      return [{'id': point.id, 'score': point.score, 'payload': point.payload} 
              for point in search_result]  # JSONå½¢å¼ã§è¿”å´
            """)

        st.markdown("**MCP (Model Context Protocol)çµŒç”±ã§ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è‡ªç„¶è¨€èªã§è³ªå•ã—ã¦ãã ã•ã„**")
        
        # MCPã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        with st.expander("ğŸ”— MCPã‚µãƒ¼ãƒãƒ¼æƒ…å ±"):
            st.markdown(f"**Qdrant MCPã‚µãƒ¼ãƒãƒ¼**: `{self.qdrant_manager.mcp_server_url}`")
            st.markdown("**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Streamlit UI â†’ OpenAI Responses API â†’ MCP Server â†’ Qdrant")
        
        # ã‚¯ã‚¨ãƒªå…¥åŠ›ã‚¨ãƒªã‚¢
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # å€™è£œãŒé¸æŠã•ã‚ŒãŸå ´åˆã€ãã®å€¤ã‚’åˆæœŸå€¤ã¨ã—ã¦ä½¿ç”¨
            initial_value = st.session_state.get('selected_suggestion', '')
            
            # è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªå…¥åŠ›
            user_query = st.text_input(
                "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                value=initial_value,
                placeholder="ä¾‹: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ä½œæ¥­ã«é©ã—ãŸãƒãƒ¼ãƒˆPCã‚’æ¢ã—ã¦",
                key="query_input"
            )
        
        with col2:
            execute_button = st.button("ğŸ” å®Ÿè¡Œ", type="primary")
        
        # ã‚¯ã‚¨ãƒªå€™è£œã®è¡¨ç¤º
        st.markdown("### ğŸ’¡ ã‚¯ã‚¨ãƒªå€™è£œ")
        suggestions = self.get_query_suggestions()
        
        # å€™è£œã‚’3åˆ—ã§è¡¨ç¤º
        cols = st.columns(3)
        for i, suggestion in enumerate(suggestions):
            with cols[i % 3]:
                if st.button(suggestion, key=f"suggestion_{i}"):
                    # å€™è£œã‚’é¸æŠã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„ã«è¨­å®šï¼ˆå®Ÿè¡Œã¯ã—ãªã„ï¼‰
                    st.session_state.selected_suggestion = suggestion
                    st.rerun()
        
        # ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆå®Ÿè¡Œãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿ï¼‰
        if execute_button and user_query:
            self.execute_mcp_query(user_query)
        elif execute_button and not user_query:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        # çµæœè¡¨ç¤º
        self.display_results()
    
    def execute_mcp_query(self, user_query: str):
        """MCPçµŒç”±ã§è‡ªç„¶è¨€èªãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        st.write(f"ğŸ” **å®Ÿè¡Œä¸­ã®ã‚¯ã‚¨ãƒª**: {user_query}")
        
        with st.spinner("ğŸ¤– MCPã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œä¸­..."):
            # MCPãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            success, results, response_message = self.query_processor.execute_mcp_query(
                user_query, 
                st.session_state.selected_model
            )
            
            if not success:
                st.error(f"MCPãƒ™ã‚¯ãƒˆãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {response_message}")
                return
            
            st.success("MCPçµŒç”±ã§ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
            
            # MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”ã‚’è¡¨ç¤º
            with st.expander("ğŸ¤– MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”", expanded=True):
                st.markdown(response_message)
            
            # çµæœã‚’ä¿å­˜
            st.session_state.current_results = results
            st.session_state.current_explanation = response_message
            st.session_state.query_history.append((user_query, "MCPçµŒç”±"))
    
    def display_results(self):
        """çµæœã®è¡¨ç¤º"""
        if st.session_state.current_results is None:
            return
        
        results = st.session_state.current_results
        
        st.markdown("---")
        st.subheader("ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ")
        
        # AI ã«ã‚ˆã‚‹èª¬æ˜
        if st.session_state.current_explanation:
            st.info(f"ğŸ¤– **AIåˆ†æ**: {st.session_state.current_explanation}")
        
        if not results:
            st.warning("æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        # çµæœã‚’ã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¡¨ç¤º
        for i, result in enumerate(results):
            with st.container():
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.write(f"**çµæœ {i+1}**")
                    payload = result.get('payload', {})
                    
                    # payloadã®å†…å®¹ã‚’è¡¨ç¤º
                    for key, value in payload.items():
                        if key in ['title', 'name', 'topic']:
                            st.write(f"**{key}**: {value}")
                        elif key in ['content', 'description']:
                            # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯çœç•¥è¡¨ç¤º
                            if len(str(value)) > 100:
                                st.write(f"**{key}**: {str(value)[:100]}...")
                            else:
                                st.write(f"**{key}**: {value}")
                        else:
                            st.write(f"**{key}**: {value}")
                
                with col2:
                    score = result.get('score', 0)
                    st.metric("é¡ä¼¼åº¦", f"{score:.3f}")
                
                st.markdown("---")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆJSONå½¢å¼ï¼‰
        if results:
            results_json = json.dumps(results, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=results_json,
                file_name=f"vector_search_results_{int(time.time())}.json",
                mime="application/json"
            )
    
    def run(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        st.set_page_config(
            page_title="MCPçµŒç”±Qdrantã‚¢ã‚¯ã‚»ã‚¹",
            page_icon="ğŸ”",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
        st.markdown("""
        <style>
        .stAlert > div {
            padding-top: 10px;
            padding-bottom: 10px;
        }
        .stButton > button {
            width: 100%;
        }
        </style>
        """, unsafe_allow_html=True)
        
        # Qdrantæ¥ç¶šçŠ¶æ…‹è¡¨ç¤ºï¼ˆMCPãƒ‡ãƒ¢ç”¨ï¼‰
        mcp_status = self._check_mcp_server_status()
        if not mcp_status:
            st.error("âš ï¸ Qdrantãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã§ãã¾ã›ã‚“")
            st.info("ğŸ’¡ **è§£æ±ºæ–¹æ³•**:\n1. `docker-compose -f docker-compose/docker-compose.mcp-demo.yml up -d qdrant` ã§Qdrantã‚’èµ·å‹•\n2. ç’°å¢ƒå¤‰æ•° `QDRANT_URL` ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã¨ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        self.create_sidebar()
        self.create_main_interface()
    
    def _check_mcp_server_status(self) -> bool:
        """Qdrantæ¥ç¶šãƒã‚§ãƒƒã‚¯ (MCPä»£æ›¿ãƒ‡ãƒ¢)"""
        try:
            if not self.qdrant_manager._qdrant_client:
                return False
            
            # ç°¡å˜ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ
            collections = self.qdrant_manager._qdrant_client.get_collections()
            return True
            
        except Exception as e:
            logger.error(f"Qdrant connection check failed: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        app = NaturalLanguageVectorInterface()
        app.run()
    except Exception as e:
        st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"Application error: {e}")


if __name__ == "__main__":
    main()