# helper_mcp_pages.py
# å„ãƒšãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®å®Œå…¨å®Ÿè£…
# ç›´æ¥ã‚¯ã‚¨ãƒªã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€è¨­å®šãƒšãƒ¼ã‚¸ã‚’å«ã‚€

import streamlit as st
import pandas as pd
import requests
import redis
import sqlalchemy
import json
import traceback
import os
from datetime import datetime
from typing import Dict, Any, List
from helper_mcp import PageManager, ServerStatusManager


# ==================================================
# ç›´æ¥ã‚¯ã‚¨ãƒªãƒšãƒ¼ã‚¸
# ==================================================
class DirectQueryPage(PageManager):
    """ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("ğŸ“Š ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª")

        query_type = st.selectbox("ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                                  ["Redis", "PostgreSQL", "Elasticsearch", "Qdrant"])

        if query_type == "Redis":
            self._render_redis_query()
        elif query_type == "PostgreSQL":
            self._render_postgresql_query()
        elif query_type == "Elasticsearch":
            self._render_elasticsearch_query()
        elif query_type == "Qdrant":
            self._render_qdrant_query()

    def _render_redis_query(self):
        """Redisã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸ”´ Redis ã‚¯ã‚¨ãƒª")

        # äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
        redis_queries = {
            "å…¨ã‚­ãƒ¼è¡¨ç¤º"  : "KEYS *",
            "ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°": "KEYS session:*",
            "ã‚«ã‚¦ãƒ³ã‚¿ä¸€è¦§": "KEYS counter:*",
            "ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤º": "SMEMBERS categories:all",
            "æ¤œç´¢å±¥æ­´"    : "LRANGE search:recent 0 -1"
        }

        col1, col2 = st.columns([1, 2])

        with col1:
            st.write("**ã‚¯ã‚¤ãƒƒã‚¯ã‚¯ã‚¨ãƒª:**")
            for name, cmd in redis_queries.items():
                if st.button(name, key=f"redis_{name}"):
                    st.session_state.redis_command = cmd

        with col2:
            redis_command = st.text_input(
                "Redisã‚³ãƒãƒ³ãƒ‰",
                value=getattr(st.session_state, 'redis_command', 'KEYS *'),
                key="redis_input"
            )

        if st.button("å®Ÿè¡Œ", key="redis_exec"):
            redis_manager = self.status_manager.get_manager('Redis')
            status = redis_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                try:
                    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
                    result = self._execute_redis_command(r, redis_command)
                    if result is not None:
                        if isinstance(result, list):
                            st.json(result)
                        else:
                            st.code(str(result))
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("Redis ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")

    def _execute_redis_command(self, redis_client, command: str):
        """Redis ã‚³ãƒãƒ³ãƒ‰ã®å®‰å…¨ãªå®Ÿè¡Œ"""
        cmd_parts = command.strip().split()
        cmd = cmd_parts[0].upper()

        if cmd == "KEYS":
            pattern = cmd_parts[1] if len(cmd_parts) > 1 else "*"
            return sorted(redis_client.keys(pattern))
        elif cmd == "GET":
            if len(cmd_parts) > 1:
                return redis_client.get(cmd_parts[1])
            else:
                st.error("GET ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "HGETALL":
            if len(cmd_parts) > 1:
                return redis_client.hgetall(cmd_parts[1])
            else:
                st.error("HGETALL ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "SMEMBERS":
            if len(cmd_parts) > 1:
                return sorted(list(redis_client.smembers(cmd_parts[1])))
            else:
                st.error("SMEMBERS ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "LRANGE":
            if len(cmd_parts) >= 4:
                key = cmd_parts[1]
                start = int(cmd_parts[2])
                stop = int(cmd_parts[3])
                return redis_client.lrange(key, start, stop)
            else:
                st.error("LRANGE ã‚³ãƒãƒ³ãƒ‰ã®å½¢å¼: LRANGE key start stop")
        else:
            st.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚³ãƒãƒ³ãƒ‰ã§ã™: {cmd}")
            st.info("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰: KEYS, GET, HGETALL, SMEMBERS, LRANGE")
        return None

    def _render_postgresql_query(self):
        """PostgreSQLã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ¦ PostgreSQL ã‚¯ã‚¨ãƒª")

        # PostgreSQLã®æ¥ç¶šçŠ¶æ…‹ç¢ºèª
        pg_manager = self.status_manager.get_manager('PostgreSQL')
        status = pg_manager.check_connection()

        if "ğŸ”´" in status["status"]:
            st.warning("PostgreSQL ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")
            st.write(f"çŠ¶æ…‹: {status['status']}")
            st.write(f"è©³ç´°: {status['details']}")
            return

        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        try:
            engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

            # åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
            available_tables = pd.read_sql("""
                                           SELECT table_name
                                           FROM information_schema.tables
                                           WHERE table_schema = 'public'
                                             AND table_type = 'BASE TABLE'
                                           ORDER BY table_name
                                           """, engine)

            table_list = available_tables['table_name'].tolist()

            engine.dispose()

            if not table_list:
                st.warning("âš ï¸ PostgreSQLã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

                with st.expander("ğŸ’¡ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †", expanded=True):
                    st.markdown("""
                    **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥:**
                    ```bash
                    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
                    uv run python scripts/setup_test_data.py
                    ```

                    **æ‰‹å‹•ã§ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¾‹:**
                    ```sql
                    -- PostgreSQLã«æ¥ç¶š
                    psql -h localhost -U testuser -d testdb

                    -- ã‚µãƒ³ãƒ—ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                    CREATE TABLE customers (
                        id SERIAL PRIMARY KEY,
                        name VARCHAR(100),
                        email VARCHAR(100),
                        city VARCHAR(50)
                    );
                    ```
                    """)
                return
            else:
                st.success(f"âœ… åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(table_list)}")

        except Exception as e:
            st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            table_list = ['customers', 'orders', 'products']  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        # äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ã‚¨ãƒªï¼ˆåˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«ã«åŸºã¥ã„ã¦å‹•çš„ç”Ÿæˆï¼‰
        pg_queries = {}

        if 'customers' in table_list:
            pg_queries.update({
                "å…¨é¡§å®¢"    : "SELECT * FROM customers ORDER BY id;",
                "æ±äº¬ã®é¡§å®¢": "SELECT * FROM customers WHERE city = 'æ±äº¬';"
            })

        if 'orders' in table_list and 'customers' in table_list:
            pg_queries.update({
                "æœ€æ–°æ³¨æ–‡": "SELECT o.*, c.name FROM orders o JOIN customers c ON o.customer_id = c.id ORDER BY o.order_date DESC LIMIT 5;",
                "å£²ä¸Šçµ±è¨ˆ": "SELECT product_name, SUM(price * quantity) as total_sales FROM orders GROUP BY product_name ORDER BY total_sales DESC;"
            })
        elif 'orders' in table_list:
            pg_queries.update({
                "æœ€æ–°æ³¨æ–‡": "SELECT * FROM orders ORDER BY id DESC LIMIT 5;",
                "å£²ä¸Šçµ±è¨ˆ": "SELECT product_name, SUM(price * quantity) as total_sales FROM orders GROUP BY product_name ORDER BY total_sales DESC;"
            })

        if 'products' in table_list:
            pg_queries["å•†å“åœ¨åº«"] = "SELECT name, stock_quantity, price FROM products ORDER BY stock_quantity DESC;"

        # ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèªã‚¯ã‚¨ãƒª
        pg_queries["ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§"] = """
                                     SELECT table_name, table_type
                                     FROM information_schema.tables
                                     WHERE table_schema = 'public'
                                     ORDER BY table_name; \
                                     """

        if table_list:
            pg_queries["ãƒ†ãƒ¼ãƒ–ãƒ«è©³ç´°"] = f"""
                SELECT 
                    column_name, 
                    data_type, 
                    is_nullable,
                    column_default
                FROM information_schema.columns 
                WHERE table_name = '{table_list[0]}'
                ORDER BY ordinal_position;
            """

        col1, col2 = st.columns([1, 2])

        with col1:
            st.write("**ã‚¯ã‚¤ãƒƒã‚¯ã‚¯ã‚¨ãƒª:**")
            for name, sql in pg_queries.items():
                if st.button(name, key=f"pg_{name}"):
                    st.session_state.sql_query = sql

        with col2:
            default_query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';"
            if table_list:
                default_query = f"SELECT * FROM {table_list[0]} LIMIT 5;"

            sql_query = st.text_area(
                "SQLã‚¯ã‚¨ãƒª",
                value=getattr(st.session_state, 'sql_query', default_query),
                height=100,
                key="pg_input",
                help="SELECTã‚¯ã‚¨ãƒªã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™"
            )

        if st.button("å®Ÿè¡Œ", key="pg_exec"):
            if not sql_query.strip().upper().startswith('SELECT'):
                st.error("âŒ å®‰å…¨æ€§ã®ãŸã‚ã€SELECTã‚¯ã‚¨ãƒªã®ã¿å®Ÿè¡Œã§ãã¾ã™")
                return

            try:
                engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

                with st.spinner("ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œä¸­..."):
                    df = pd.read_sql(sql_query, engine)

                if len(df) > 0:
                    st.success(f"âœ… {len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
                    st.dataframe(df, use_container_width=True)

                    # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±ã®è¡¨ç¤º
                    with st.expander("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±", expanded=False):
                        dtype_info = pd.DataFrame({
                            'ã‚«ãƒ©ãƒ å'  : df.columns,
                            'ãƒ‡ãƒ¼ã‚¿å‹'  : [str(dtype) for dtype in df.dtypes],
                            'NULLæ•°'    : [df[col].isnull().sum() for col in df.columns],
                            'ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°': [df[col].nunique() for col in df.columns]
                        })
                        st.dataframe(dtype_info, use_container_width=True)

                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
                    csv = df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name=f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime='text/csv',
                        help="ã‚¯ã‚¨ãƒªçµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™"
                    )
                else:
                    st.info("ã‚¯ã‚¨ãƒªã®çµæœã¯ç©ºã§ã—ãŸ")

                engine.dispose()

            except Exception as e:
                st.error(f"âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±
                with st.expander("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                    st.code(f"SQL: {sql_query}")
                    st.code(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")

                    # ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã®ãƒ’ãƒ³ãƒˆ
                    error_str = str(e).lower()
                    if "does not exist" in error_str:
                        st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¾ãŸã¯ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã€Œãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã€ã‚¯ã‚¨ãƒªã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    elif "syntax error" in error_str:
                        st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: SQLæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚ã‚¯ã‚¨ãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    elif "permission denied" in error_str:
                        st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: æ¨©é™ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¨©é™è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    def _render_elasticsearch_query(self):
        """Elasticsearchã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ¡ Elasticsearch ã‚¯ã‚¨ãƒª")

        search_term = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "Python")
        search_field = st.selectbox("æ¤œç´¢å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", ["å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", "title", "content", "category", "author"])

        if st.button("æ¤œç´¢å®Ÿè¡Œ", key="es_exec"):
            es_manager = self.status_manager.get_manager('Elasticsearch')
            status = es_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                hits = es_manager.search_articles(search_term, search_field)

                if hits:
                    st.success(f"ğŸ¯ {len(hits)}ä»¶ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                    for hit in hits:
                        article = hit['_source']
                        score = hit['_score']

                        with st.expander(f"ğŸ“ {article['title']} (ã‚¹ã‚³ã‚¢: {score:.2f})"):
                            col1, col2 = st.columns([3, 1])

                            with col1:
                                st.write(f"**å†…å®¹:** {article['content']}")

                                # ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                                if 'highlight' in hit:
                                    st.write("**ãƒã‚¤ãƒ©ã‚¤ãƒˆ:**")
                                    for field, highlights in hit['highlight'].items():
                                        for highlight in highlights:
                                            st.markdown(f"â€¢ {highlight}", unsafe_allow_html=True)

                            with col2:
                                st.metric("é–²è¦§æ•°", f"{article['view_count']:,}")
                                st.write(f"**è‘—è€…:** {article['author']}")
                                st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {article['category']}")
                                st.write(f"**å…¬é–‹æ—¥:** {article['published_date']}")
                                st.write(f"**ã‚¿ã‚°:** {', '.join(article['tags'])}")
                else:
                    st.info(f"'{search_term}' ã«é–¢ã™ã‚‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.warning("Elasticsearch ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")

    def _render_qdrant_query(self):
        """Qdrantã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ  Qdrant ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢")

        st.info("ğŸ’¡ å®Ÿéš›ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã¯åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™ãŒã€ã“ã“ã§ã¯ãƒ†ã‚¹ãƒˆç”¨ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™")

        col1, col2 = st.columns(2)

        with col1:
            search_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã§æ¤œç´¢",
                                           ["å…¨ã¦", "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹", "ã‚­ãƒƒãƒãƒ³å®¶é›»", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "ã‚¹ãƒãƒ¼ãƒ„"])
            price_range = st.slider("ä¾¡æ ¼å¸¯", 0, 100000, (0, 100000), step=1000)

        with col2:
            limit = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=20, value=5)

        if st.button("æ¤œç´¢å®Ÿè¡Œ", key="qdrant_exec"):
            qdrant_manager = self.status_manager.get_manager('Qdrant')
            status = qdrant_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                try:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æ§‹ç¯‰
                    filter_conditions = []

                    if search_category != "å…¨ã¦":
                        filter_conditions.append({
                            "key"  : "category",
                            "match": {"value": search_category}
                        })

                    filter_conditions.extend([
                        {"key": "price", "range": {"gte": price_range[0]}},
                        {"key": "price", "range": {"lte": price_range[1]}}
                    ])

                    # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                    search_request = {
                        "filter"      : {
                            "must": filter_conditions
                        },
                        "limit"       : limit,
                        "with_payload": True
                    }

                    response = requests.post(
                        'http://localhost:6333/collections/product_embeddings/points/search',
                        json=search_request,
                        headers={'Content-Type': 'application/json'}
                    )

                    if response.status_code == 200:
                        data = response.json()
                        if 'result' in data and data['result']:
                            points = data['result']

                            st.success(f"ğŸ¯ {len(points)}ä»¶ã®å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                            # å•†å“ä¸€è¦§è¡¨ç¤º
                            products = []
                            for point in points:
                                product = point['payload']
                                product['id'] = point['id']
                                if 'score' in point:
                                    product['similarity_score'] = point['score']
                                products.append(product)

                            df_results = pd.DataFrame(products)
                            st.dataframe(df_results, use_container_width=True)

                            # å•†å“è©³ç´°ã‚«ãƒ¼ãƒ‰
                            for product in products:
                                with st.expander(f"ğŸ›ï¸ {product['name']} - Â¥{product['price']:,}"):
                                    col1, col2 = st.columns([2, 1])

                                    with col1:
                                        st.write(f"**èª¬æ˜:** {product['description']}")
                                        st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {product['category']}")

                                    with col2:
                                        st.metric("ä¾¡æ ¼", f"Â¥{product['price']:,}")
                                        if 'similarity_score' in product:
                                            st.metric("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢", f"{product['similarity_score']:.3f}")
                        else:
                            st.info("æ¡ä»¶ã«åˆã†å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    else:
                        st.error(f"æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ (Status: {response.status_code})")

                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("Qdrant ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")


# ==================================================
# ãƒ‡ãƒ¼ã‚¿åˆ†æãƒšãƒ¼ã‚¸
# ==================================================
class DataAnalysisPage(PageManager):
    """ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†æã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

        # å¿…è¦ãªã‚µãƒ¼ãƒãƒ¼ã®ç¢ºèª
        status = self.status_manager.check_all_servers()
        required_servers = ['PostgreSQL', 'Redis']
        servers_ready = all("ğŸŸ¢" in status[server]["status"] for server in required_servers)

        if not servers_ready:
            st.warning("ãƒ‡ãƒ¼ã‚¿åˆ†æã«ã¯ PostgreSQL ã¨ Redis ã®æ¥ç¶šãŒå¿…è¦ã§ã™")

            # æ¥ç¶šçŠ¶æ³ã®è©³ç´°è¡¨ç¤º
            st.write("**ç¾åœ¨ã®æ¥ç¶šçŠ¶æ³:**")
            for server in required_servers:
                if server in status:
                    st.write(f"- {server}: {status[server]['status']}")
                else:
                    st.write(f"- {server}: çŠ¶æ…‹ä¸æ˜")
            return

        try:
            # PostgreSQLã®ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not self._check_postgresql_setup():
                return

            self._render_sales_analysis()
            self._render_customer_analysis()
            self._render_redis_statistics()
            self._render_advanced_analytics()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            st.code(traceback.format_exc())

    def _check_postgresql_setup(self) -> bool:
        """PostgreSQLã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
            tables_check = pd.read_sql("""
                                       SELECT table_name
                                       FROM information_schema.tables
                                       WHERE table_schema = 'public'
                                         AND table_type = 'BASE TABLE'
                                         AND table_name IN ('customers', 'orders', 'products')
                                       """, engine)

            existing_tables = set(tables_check['table_name'].tolist())
            required_tables = {'customers', 'orders', 'products'}
            missing_tables = required_tables - existing_tables

            engine.dispose()

            if missing_tables:
                st.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿åˆ†æã«å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing_tables)}")

                # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã®è¡¨ç¤º
                with st.expander("ğŸ’¡ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †", expanded=True):
                    st.markdown("""
                    **PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥:**

                    1. **Dockerã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•:**
                    ```bash
                    docker-compose -f docker-compose.mcp-demo.yml up -d postgres
                    ```

                    2. **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥:**
                    ```bash
                    uv run python scripts/setup_test_data.py
                    ```

                    3. **æ‰‹å‹•ã§ã®ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª:**
                    ```sql
                    -- PostgreSQLã«æ¥ç¶š
                    psql -h localhost -U testuser -d testdb

                    -- ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§è¡¨ç¤º
                    \dt

                    -- ãƒ‡ãƒ¼ã‚¿ç¢ºèª
                    SELECT COUNT(*) FROM customers;
                    SELECT COUNT(*) FROM orders;
                    SELECT COUNT(*) FROM products;
                    ```
                    """)

                # ç¾åœ¨ã®çŠ¶æ³è¡¨ç¤º
                if existing_tables:
                    st.info(f"âœ… å­˜åœ¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(existing_tables)}")
                else:
                    st.error("âŒ PostgreSQLã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒ1ã¤ã‚‚å­˜åœ¨ã—ã¾ã›ã‚“")

                return False
            else:
                st.success(f"âœ… å…¨ã¦ã®å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã™: {', '.join(existing_tables)}")
                return True

        except Exception as e:
            st.error(f"PostgreSQLã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

            with st.expander("ğŸ” è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.code(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                st.code(f"æ¥ç¶šæ–‡å­—åˆ—: {os.getenv('PG_CONN_STR', 'æœªè¨­å®š')}")

            return False

    def _table_exists(self, engine, table_name: str) -> bool:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        try:
            result = pd.read_sql(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """, engine)
            return result.iloc[0]['exists']
        except Exception:
            return False

    def _render_sales_analysis(self):
        """å£²ä¸Šåˆ†æ"""
        st.subheader("ğŸ’° å£²ä¸Šåˆ†æ")

        try:
            engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            if not self._table_exists(engine, 'orders'):
                st.warning("ordersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å£²ä¸Šåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                engine.dispose()
                return

            col1, col2, col3 = st.columns(3)

            # ç·å£²ä¸Š
            try:
                total_sales = pd.read_sql("SELECT SUM(price * quantity) as total FROM orders", engine).iloc[0]['total']
                with col1:
                    if total_sales is not None:
                        st.metric("ç·å£²ä¸Š", f"Â¥{total_sales:,.0f}")
                    else:
                        st.metric("ç·å£²ä¸Š", "Â¥0")
            except Exception as e:
                with col1:
                    st.metric("ç·å£²ä¸Š", f"ã‚¨ãƒ©ãƒ¼: {str(e)[:20]}")

            # å¹³å‡æ³¨æ–‡ä¾¡æ ¼
            try:
                avg_order = pd.read_sql("SELECT AVG(price * quantity) as avg FROM orders", engine).iloc[0]['avg']
                with col2:
                    if avg_order is not None:
                        st.metric("å¹³å‡æ³¨æ–‡ä¾¡æ ¼", f"Â¥{avg_order:,.0f}")
                    else:
                        st.metric("å¹³å‡æ³¨æ–‡ä¾¡æ ¼", "Â¥0")
            except Exception as e:
                with col2:
                    st.metric("å¹³å‡æ³¨æ–‡ä¾¡æ ¼", f"ã‚¨ãƒ©ãƒ¼: {str(e)[:20]}")

            # æ³¨æ–‡æ•°
            try:
                order_count = pd.read_sql("SELECT COUNT(*) as count FROM orders", engine).iloc[0]['count']
                with col3:
                    st.metric("ç·æ³¨æ–‡æ•°", f"{order_count:,}")
            except Exception as e:
                with col3:
                    st.metric("ç·æ³¨æ–‡æ•°", f"ã‚¨ãƒ©ãƒ¼: {str(e)[:20]}")

            # å•†å“åˆ¥å£²ä¸Š
            try:
                st.subheader("ğŸ“Š å•†å“åˆ¥å£²ä¸Š")
                product_sales = pd.read_sql("""
                                            SELECT product_name,
                                                   SUM(price * quantity) as total_sales,
                                                   COUNT(*)              as order_count,
                                                   AVG(price * quantity) as avg_order_value
                                            FROM orders
                                            GROUP BY product_name
                                            ORDER BY total_sales DESC
                                            """, engine)

                if len(product_sales) > 0:
                    col1, col2 = st.columns(2)

                    with col1:
                        st.write("**å£²ä¸Šé‡‘é¡**")
                        st.bar_chart(product_sales.set_index('product_name')['total_sales'])

                    with col2:
                        st.write("**æ³¨æ–‡ä»¶æ•°**")
                        st.bar_chart(product_sales.set_index('product_name')['order_count'])

                    # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
                    st.write("**å•†å“åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿**")
                    st.dataframe(product_sales, use_container_width=True)
                else:
                    st.info("å•†å“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            except Exception as e:
                st.error(f"å•†å“åˆ¥å£²ä¸Šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

            engine.dispose()

        except Exception as e:
            st.error(f"å£²ä¸Šåˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _render_customer_analysis(self):
        """é¡§å®¢åˆ†æ"""
        st.subheader("ğŸ‘¥ é¡§å®¢åˆ†æ")

        try:
            engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

            # customersãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not self._table_exists(engine, 'customers'):
                st.warning("customersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€é¡§å®¢åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                engine.dispose()
                return

            # ordersãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
            orders_exists = self._table_exists(engine, 'orders')

            if orders_exists:
                # ä¸¡æ–¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                customer_stats = pd.read_sql("""
                                             SELECT c.city,
                                                    COUNT(c.id)                            as customer_count,
                                                    COUNT(o.id)                            as total_orders,
                                                    COALESCE(SUM(o.price * o.quantity), 0) as total_spent,
                                                    COALESCE(AVG(o.price * o.quantity), 0) as avg_order_value
                                             FROM customers c
                                                      LEFT JOIN orders o ON c.id = o.customer_id
                                             GROUP BY c.city
                                             ORDER BY total_spent DESC
                                             """, engine)
            else:
                # customersãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿å­˜åœ¨ã™ã‚‹å ´åˆ
                customer_stats = pd.read_sql("""
                                             SELECT city,
                                                    COUNT(id) as customer_count,
                                                    0         as total_orders,
                                                    0         as total_spent,
                                                    0         as avg_order_value
                                             FROM customers
                                             GROUP BY city
                                             ORDER BY customer_count DESC
                                             """, engine)

            if len(customer_stats) > 0:
                col1, col2 = st.columns(2)

                with col1:
                    st.write("**éƒ½å¸‚åˆ¥é¡§å®¢æ•°**")
                    st.bar_chart(customer_stats.set_index('city')['customer_count'])

                with col2:
                    if orders_exists:
                        st.write("**éƒ½å¸‚åˆ¥å£²ä¸Š**")
                        st.bar_chart(customer_stats.set_index('city')['total_spent'])
                    else:
                        st.write("**å£²ä¸Šãƒ‡ãƒ¼ã‚¿ãªã—**")
                        st.info("ordersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“")

                # é¡§å®¢åˆ†æè©³ç´°
                st.write("**éƒ½å¸‚åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿**")
                st.dataframe(customer_stats, use_container_width=True)

                # ä¸Šä½é¡§å®¢åˆ†æï¼ˆordersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
                if orders_exists:
                    top_customers = pd.read_sql("""
                                                SELECT c.name,
                                                       c.city,
                                                       c.email,
                                                       COUNT(o.id)               as order_count,
                                                       SUM(o.price * o.quantity) as total_spent
                                                FROM customers c
                                                         JOIN orders o ON c.id = o.customer_id
                                                GROUP BY c.id, c.name, c.city, c.email
                                                ORDER BY total_spent DESC
                                                LIMIT 10
                                                """, engine)

                    if len(top_customers) > 0:
                        st.write("**ä¸Šä½é¡§å®¢ Top 10**")
                        st.dataframe(top_customers, use_container_width=True)
            else:
                st.info("é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            engine.dispose()

        except Exception as e:
            st.error(f"é¡§å®¢åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _render_redis_statistics(self):
        """Redisçµ±è¨ˆ"""
        st.subheader("ğŸ”´ Redis çµ±è¨ˆ")

        try:
            r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

            col1, col2, col3 = st.columns(3)

            with col1:
                active_sessions = len(r.keys('session:*'))
                st.metric("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³", active_sessions)

            with col2:
                page_views = r.get('counter:page_views') or 0
                st.metric("ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼", f"{page_views:,}")

            with col3:
                search_count = r.llen('search:recent')
                st.metric("æ¤œç´¢å±¥æ­´æ•°", search_count)

            # Redisè©³ç´°çµ±è¨ˆ
            st.write("**Redisè©³ç´°çµ±è¨ˆ**")

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆinfo commandã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ï¼‰
            redis_info = r.info()

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                used_memory = redis_info.get('used_memory_human', 'N/A')
                st.metric("ä½¿ç”¨ãƒ¡ãƒ¢ãƒª", used_memory)

            with col2:
                connected_clients = redis_info.get('connected_clients', 0)
                st.metric("æ¥ç¶šã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°", connected_clients)

            with col3:
                total_commands = redis_info.get('total_commands_processed', 0)
                st.metric("ç·ã‚³ãƒãƒ³ãƒ‰æ•°", f"{total_commands:,}")

            with col4:
                uptime_days = redis_info.get('uptime_in_days', 0)
                st.metric("ç¨¼åƒæ—¥æ•°", f"{uptime_days}æ—¥")

        except Exception as e:
            st.error(f"Redisçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    def _render_advanced_analytics(self):
        """é«˜åº¦ãªåˆ†æ"""
        st.subheader("ğŸ”¬ é«˜åº¦ãªåˆ†æ")

        try:
            engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

            # ordersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not self._table_exists(engine, 'orders'):
                st.warning("ordersãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€é«˜åº¦ãªåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                engine.dispose()
                return

            # æ™‚ç³»åˆ—åˆ†æï¼ˆã‚‚ã—order_dateãŒã‚ã‚Œã°ï¼‰
            try:
                daily_sales = pd.read_sql("""
                                          SELECT DATE(order_date)      as date,
                                                 COUNT(*)              as order_count,
                                                 SUM(price * quantity) as daily_sales
                                          FROM orders
                                          WHERE order_date IS NOT NULL
                                          GROUP BY DATE(order_date)
                                          ORDER BY date
                                          """, engine)

                if len(daily_sales) > 0:
                    st.write("**æ—¥æ¬¡å£²ä¸Šæ¨ç§»**")
                    st.line_chart(daily_sales.set_index('date')['daily_sales'])

                    st.write("**æ—¥æ¬¡æ³¨æ–‡æ•°æ¨ç§»**")
                    st.line_chart(daily_sales.set_index('date')['order_count'])
                else:
                    st.info("æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆorder_dateã‚«ãƒ©ãƒ ãŒç©ºã¾ãŸã¯NULLï¼‰")
            except Exception as e:
                st.info(f"æ™‚ç³»åˆ—åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ: {str(e)[:50]}")

            # å•†å“ã‚«ãƒ†ã‚´ãƒªåˆ†æ
            try:
                # productsãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
                if self._table_exists(engine, 'products'):
                    category_analysis = pd.read_sql("""
                                                    SELECT p.category,
                                                           COUNT(DISTINCT p.id)                   as product_count,
                                                           COALESCE(SUM(o.price * o.quantity), 0) as total_sales,
                                                           COALESCE(COUNT(o.id), 0)               as order_count
                                                    FROM products p
                                                             LEFT JOIN orders o ON p.name = o.product_name
                                                    GROUP BY p.category
                                                    ORDER BY total_sales DESC
                                                    """, engine)

                    if len(category_analysis) > 0:
                        st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ**")
                        st.dataframe(category_analysis, use_container_width=True)
                    else:
                        st.info("ã‚«ãƒ†ã‚´ãƒªåˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    st.info("productsãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            except Exception as e:
                st.info(f"ã‚«ãƒ†ã‚´ãƒªåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ: {str(e)[:50]}")

            engine.dispose()

        except Exception as e:
            st.error(f"é«˜åº¦ãªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")


# ==================================================
# è¨­å®šãƒšãƒ¼ã‚¸
# ==================================================
class SettingsPage(PageManager):
    """è¨­å®šã¨ãƒ˜ãƒ«ãƒ—ãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("âš™ï¸ è¨­å®šã¨ãƒ˜ãƒ«ãƒ—")

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self._render_system_info()

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        self._render_performance_info()

        # Docker ã‚³ãƒãƒ³ãƒ‰
        self._render_docker_commands()

        # MCP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
        self._render_mcp_endpoints()

        # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        self._render_troubleshooting()

        # ã‚¢ãƒ—ãƒªæƒ…å ±
        self._render_app_info()

    def _render_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**ç’°å¢ƒå¤‰æ•°:**")
            env_status = {
                "OPENAI_API_KEY": "è¨­å®šæ¸ˆã¿" if os.getenv('OPENAI_API_KEY') else "âŒ æœªè¨­å®š",
                "REDIS_URL"     : os.getenv('REDIS_URL', 'æœªè¨­å®š'),
                "PG_CONN_STR"   : "è¨­å®šæ¸ˆã¿" if os.getenv('PG_CONN_STR') else "âŒ æœªè¨­å®š",
                "ELASTIC_URL"   : os.getenv('ELASTIC_URL', 'http://localhost:9200'),
                "QDRANT_URL"    : os.getenv('QDRANT_URL', 'http://localhost:6333')
            }

            for key, value in env_status.items():
                if "âŒ" in str(value):
                    st.error(f"**{key}**: {value}")
                else:
                    st.success(f"**{key}**: {value}")

        with col2:
            st.write("**ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šçŠ¶æ³:**")
            status = self.status_manager.check_all_servers()
            for server, state in status.items():
                if "ğŸŸ¢" in state["status"]:
                    st.success(f"**{server}**: æ¥ç¶šOK")
                    if "details" in state:
                        st.caption(f"è©³ç´°: {state['details']}")
                else:
                    st.error(f"**{server}**: æ¥ç¶šNG")
                    if "details" in state:
                        st.caption(f"è©³ç´°: {state['details']}")

    def _render_performance_info(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚µã‚¤ã‚º
        session_size = len(st.session_state)
        st.metric("ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°æ•°", session_size)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        if hasattr(st, 'cache_data'):
            st.info("Streamlitã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã™")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", f"{memory_mb:.1f} MB")
        except ImportError:
            st.info("è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã«ã¯psutilãŒå¿…è¦ã§ã™")
        except Exception:
            st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    def _render_docker_commands(self):
        """Dockerç®¡ç†ã‚³ãƒãƒ³ãƒ‰ã®è¡¨ç¤º"""
        st.subheader("ğŸ³ Docker ç®¡ç†ã‚³ãƒãƒ³ãƒ‰")

        command_tabs = st.tabs(["èµ·å‹•", "åœæ­¢", "ãƒ­ã‚°ç¢ºèª", "ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ"])

        with command_tabs[0]:
            st.write("**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d redis postgres elasticsearch qdrant")

            st.write("**MCPã‚µãƒ¼ãƒãƒ¼èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d redis-mcp postgres-mcp es-mcp qdrant-mcp")

            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d")

        with command_tabs[1]:
            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml down")

            st.write("**ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ï¼‰:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml down -v")

        with command_tabs[2]:
            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml logs -f")

            st.write("**ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml logs -f redis-mcp")

        with command_tabs[3]:
            st.write("**ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å†æŠ•å…¥:**")
            st.code("uv run python scripts/setup_test_data.py")

            st.write("**å®Œå…¨ãƒªã‚»ãƒƒãƒˆ:**")
            st.code("""
# åœæ­¢ã—ã¦ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤  
docker-compose -f docker-compose.mcp-demo.yml down -v

# å†èµ·å‹•
docker-compose -f docker-compose.mcp-demo.yml up -d

# ãƒ‡ãƒ¼ã‚¿å†æŠ•å…¥
uv run python scripts/setup_test_data.py
            """)

    def _render_mcp_endpoints(self):
        """MCPã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("ğŸŒ MCP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ")

        mcp_endpoints = {
            "Redis MCP"        : "http://localhost:8000/mcp",
            "PostgreSQL MCP"   : "http://localhost:8001/mcp",
            "Elasticsearch MCP": "http://localhost:8002/mcp",
            "Qdrant MCP"       : "http://localhost:8003/mcp"
        }

        st.json(mcp_endpoints)

        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        st.write("**ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯:**")
        for name, url in mcp_endpoints.items():
            try:
                response = requests.get(url, timeout=3)
                if response.status_code == 200:
                    st.success(f"{name}: ğŸŸ¢ å¿œç­”OK")
                else:
                    st.warning(f"{name}: ğŸŸ¡ å¿œç­”ã‚ã‚Š (Status: {response.status_code})")
            except Exception:
                st.error(f"{name}: ğŸ”´ å¿œç­”ãªã—")

    def _render_troubleshooting(self):
        """ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®è¡¨ç¤º"""
        st.subheader("ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°")

        with st.expander("â“ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•"):
            st.markdown("""
            **ğŸ”´ Redis æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: `docker ps | grep redis`
            - ãƒãƒ¼ãƒˆ6379ãŒä½¿ç”¨ä¸­ã§ãªã„ã‹ç¢ºèª: `lsof -i :6379`
            - Redisè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª: `docker logs redis`

            **ğŸŸ¦ PostgreSQL æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - èªè¨¼æƒ…å ±ã‚’ç¢ºèª: `testuser/testpass`
            - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚’ç¢ºèª: `docker-compose logs postgres`
            - æ¥ç¶šæ–‡å­—åˆ—ã®ç¢ºèª: `PG_CONN_STR`ç’°å¢ƒå¤‰æ•°

            **ğŸŸ¡ Elasticsearch æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§: `docker stats`
            - Java heap sizeè¨­å®šã‚’ç¢ºèª: `ES_JAVA_OPTS=-Xms512m -Xmx512m`
            - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª: `curl http://localhost:9200/_cat/indices`

            **ğŸŸ  Qdrant æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•çŠ¶æ³: `docker-compose ps qdrant`
            - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: `curl http://localhost:6333/`
            - ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª: `curl http://localhost:6333/collections`

            **ğŸ¤– OpenAI API ã‚¨ãƒ©ãƒ¼**
            - APIã‚­ãƒ¼ã®è¨­å®šç¢ºèª: `.env`ãƒ•ã‚¡ã‚¤ãƒ«ã®`OPENAI_API_KEY`
            - ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã®ç¢ºèª: OpenAIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
            - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®ç¢ºèª

            **ğŸ³ Dockeré–¢é€£ã‚¨ãƒ©ãƒ¼**
            - Docker Daemonã®èµ·å‹•ç¢ºèª: `docker info`
            - ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ç¢ºèª: `docker system df`
            - ãƒãƒ¼ãƒˆç«¶åˆã®ç¢ºèª: `netstat -tulpn | grep :6379`

            **ğŸ’¾ ãƒ‡ãƒ¼ã‚¿é–¢é€£ã‚¨ãƒ©ãƒ¼**
            - ãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–: `scripts/setup_test_data.py`ã®å®Ÿè¡Œ
            - ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®å†ä½œæˆ: `docker-compose down -v && docker-compose up -d`
            """)

        # è‡ªå‹•è¨ºæ–­æ©Ÿèƒ½
        with st.expander("ğŸ” è‡ªå‹•è¨ºæ–­"):
            if st.button("ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œ"):
                self._run_system_diagnosis()

    def _run_system_diagnosis(self):
        """ã‚·ã‚¹ãƒ†ãƒ è‡ªå‹•è¨ºæ–­"""
        st.write("**ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œä¸­...**")

        diagnosis_results = []

        # Dockerç¢ºèª
        try:
            import subprocess
            result = subprocess.run(['docker', '--version'],
                                    capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                diagnosis_results.append("âœ… Docker: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
            else:
                diagnosis_results.append("âŒ Docker: å•é¡Œã‚ã‚Š")
        except Exception:
            diagnosis_results.append("âŒ Docker: ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
        status = self.status_manager.check_all_servers()
        for server, state in status.items():
            if "ğŸŸ¢" in state["status"]:
                diagnosis_results.append(f"âœ… {server}: æ¥ç¶šOK")
            else:
                diagnosis_results.append(f"âŒ {server}: æ¥ç¶šNG")

        # ç’°å¢ƒå¤‰æ•°ç¢ºèª
        required_env_vars = ['OPENAI_API_KEY', 'PG_CONN_STR']
        for var in required_env_vars:
            if os.getenv(var):
                diagnosis_results.append(f"âœ… {var}: è¨­å®šæ¸ˆã¿")
            else:
                diagnosis_results.append(f"âŒ {var}: æœªè¨­å®š")

        # çµæœè¡¨ç¤º
        for result in diagnosis_results:
            if "âœ…" in result:
                st.success(result)
            else:
                st.error(result)

    def _render_app_info(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("â„¹ï¸ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±")

        app_info = {
            "ãƒãƒ¼ã‚¸ãƒ§ãƒ³"    : "2.0.0-refactored",
            "ä½œæˆæ—¥"        : "2024-01-15",
            "æœ€çµ‚æ›´æ–°"      : "2025-01-28",
            "Python"        : "3.11+",
            "Streamlit"     : st.__version__,
            "ä½¿ç”¨æŠ€è¡“"      : ["Docker", "Redis", "PostgreSQL", "Elasticsearch", "Qdrant", "OpenAI API"],
            "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£": "ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ",
            "ä¸»è¦æ”¹å–„ç‚¹"    : [
                "1,100è¡Œã‹ã‚‰50è¡Œã®ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«å‰Šæ¸›",
                "ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®è¨­è¨ˆ",
                "æ©Ÿèƒ½åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†å‰²",
                "ä¿å®ˆæ€§ãƒ»æ‹¡å¼µæ€§ã®å‘ä¸Š"
            ]
        }

        st.json(app_info)

        # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±
        with st.expander("ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»è‘—ä½œæ¨©æƒ…å ±"):
            st.markdown("""
            **MIT License**

            Copyright (c) 2025 MCP Demo Application

            æœ¬ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãŠã‚ˆã³é–¢é€£æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»¥ä¸‹ã€Œã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã€ï¼‰ã®ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã™ã‚‹
            ã™ã¹ã¦ã®äººã«å¯¾ã—ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ç„¡åˆ¶é™ã«æ‰±ã†ã“ã¨ã‚’ç„¡å„Ÿã§è¨±å¯ã—ã¾ã™ã€‚

            **ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:**
            - Streamlit (Apache License 2.0)
            - Pandas (BSD License)
            - Redis-py (MIT License)
            - SQLAlchemy (MIT License)
            - ãã®ä»–requirements.txtã«è¨˜è¼‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
            """)


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==================================================
__all__ = [
    'DirectQueryPage',
    'DataAnalysisPage',
    'SettingsPage',
]
